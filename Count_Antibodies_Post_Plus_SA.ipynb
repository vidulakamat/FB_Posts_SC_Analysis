{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Count_Antibodies_Post_Plus_SA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmBNIwVFQlsBg06SgAG94i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidulakamat/FB_Posts_SC_Analysis/blob/main/Count_Antibodies_Post_Plus_SA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-LSPeJ1Nlpy"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yavxSfzpN4yL"
      },
      "source": [
        "import re\r\n",
        "import spacy\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXjAa0BUA8t9"
      },
      "source": [
        "import collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfw7cK4lBJ3r"
      },
      "source": [
        "# Import stopwords\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "# Import textblob\r\n",
        "from textblob import Word, TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MICcvNnqN7Fe",
        "outputId": "8dabce1d-e74b-43aa-a3b1-dd8bcbfab0f4"
      },
      "source": [
        "pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.6/dist-packages (0.4.1.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.1.95)\n",
            "Collecting transformers<5.0.0,>=3.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Using cached https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "\u001b[31mERROR: aspect-based-sentiment-analysis 2.0.2 has requirement transformers==2.5, but you'll have transformers 4.3.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.5.0\n",
            "    Uninstalling tokenizers-0.5.0:\n",
            "      Successfully uninstalled tokenizers-0.5.0\n",
            "  Found existing installation: transformers 2.5.0\n",
            "    Uninstalling transformers-2.5.0:\n",
            "      Successfully uninstalled transformers-2.5.0\n",
            "Successfully installed tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob6_AlZQE55C",
        "outputId": "991eb293-7886-4ae2-b6ff-78fc086c2200"
      },
      "source": [
        "pip install kneed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kneed in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from kneed) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.6/dist-packages (from kneed) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from kneed) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->kneed) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->kneed) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DSPN9IC55KK",
        "outputId": "a23fd8b0-8868-4c68-d9fe-bf4748eb3c92"
      },
      "source": [
        "pip install aspect-based-sentiment-analysis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aspect-based-sentiment-analysis in /usr/local/lib/python3.6/dist-packages (2.0.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from aspect-based-sentiment-analysis) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from aspect-based-sentiment-analysis) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow==2.2 in /usr/local/lib/python3.6/dist-packages (from aspect-based-sentiment-analysis) (2.2.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (from aspect-based-sentiment-analysis) (1.18.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from aspect-based-sentiment-analysis) (5.5.0)\n",
            "Requirement already satisfied: testfixtures in /usr/local/lib/python3.6/dist-packages (from aspect-based-sentiment-analysis) (6.17.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from aspect-based-sentiment-analysis) (3.6.4)\n",
            "Collecting transformers==2.5\n",
            "  Using cached https://files.pythonhosted.org/packages/04/58/3d789b98923da6485f376be1e04d59ad7003a63bdb2b04b5eea7e02857e5/transformers-2.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.6/dist-packages (from aspect-based-sentiment-analysis) (2.5.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (53.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->aspect-based-sentiment-analysis) (1.0.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->aspect-based-sentiment-analysis) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->aspect-based-sentiment-analysis) (1.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (1.32.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (3.12.4)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (2.10.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (2.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (0.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2->aspect-based-sentiment-analysis) (0.36.2)\n",
            "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->aspect-based-sentiment-analysis) (1.25.0)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->aspect-based-sentiment-analysis) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->aspect-based-sentiment-analysis) (1.0.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->aspect-based-sentiment-analysis) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->aspect-based-sentiment-analysis) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->aspect-based-sentiment-analysis) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->aspect-based-sentiment-analysis) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->aspect-based-sentiment-analysis) (4.3.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->aspect-based-sentiment-analysis) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->aspect-based-sentiment-analysis) (0.8.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->aspect-based-sentiment-analysis) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->aspect-based-sentiment-analysis) (8.7.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->aspect-based-sentiment-analysis) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->aspect-based-sentiment-analysis) (20.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->aspect-based-sentiment-analysis) (1.10.0)\n",
            "Collecting tokenizers==0.5.0\n",
            "  Using cached https://files.pythonhosted.org/packages/7e/1d/ea7e2c628942e686595736f73678348272120d026b7acd54fe43e5211bb1/tokenizers-0.5.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.5->aspect-based-sentiment-analysis) (0.0.43)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5->aspect-based-sentiment-analysis) (1.17.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5->aspect-based-sentiment-analysis) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.5->aspect-based-sentiment-analysis) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.5->aspect-based-sentiment-analysis) (0.1.95)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from optuna->aspect-based-sentiment-analysis) (4.7.2)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from optuna->aspect-based-sentiment-analysis) (1.5.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna->aspect-based-sentiment-analysis) (20.9)\n",
            "Requirement already satisfied: cmaes>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from optuna->aspect-based-sentiment-analysis) (0.8.2)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.6/dist-packages (from optuna->aspect-based-sentiment-analysis) (3.7.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna->aspect-based-sentiment-analysis) (1.3.23)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->aspect-based-sentiment-analysis) (3.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->aspect-based-sentiment-analysis) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->aspect-based-sentiment-analysis) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->aspect-based-sentiment-analysis) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->aspect-based-sentiment-analysis) (2020.12.5)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->aspect-based-sentiment-analysis) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->aspect-based-sentiment-analysis) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->aspect-based-sentiment-analysis) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->aspect-based-sentiment-analysis) (3.3.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->aspect-based-sentiment-analysis) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->aspect-based-sentiment-analysis) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->aspect-based-sentiment-analysis) (4.7)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->aspect-based-sentiment-analysis) (1.16.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->aspect-based-sentiment-analysis) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->aspect-based-sentiment-analysis) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->aspect-based-sentiment-analysis) (0.2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5->aspect-based-sentiment-analysis) (7.1.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.12 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5->aspect-based-sentiment-analysis) (1.20.12)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5->aspect-based-sentiment-analysis) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5->aspect-based-sentiment-analysis) (0.3.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna->aspect-based-sentiment-analysis) (2.8.1)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->optuna->aspect-based-sentiment-analysis) (1.0.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->optuna->aspect-based-sentiment-analysis) (1.1.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->optuna->aspect-based-sentiment-analysis) (2.4.7)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->aspect-based-sentiment-analysis) (3.13)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->aspect-based-sentiment-analysis) (2.0.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->aspect-based-sentiment-analysis) (1.5.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->aspect-based-sentiment-analysis) (3.3.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->aspect-based-sentiment-analysis) (5.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->aspect-based-sentiment-analysis) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->aspect-based-sentiment-analysis) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->aspect-based-sentiment-analysis) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage->aspect-based-sentiment-analysis) (0.4.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->aspect-based-sentiment-analysis) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->aspect-based-sentiment-analysis) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna->aspect-based-sentiment-analysis) (1.1.1)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.6/dist-packages (from cmd2>=1.0.0->cliff->optuna->aspect-based-sentiment-analysis) (1.8.2)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.6/dist-packages (from cmd2>=1.0.0->cliff->optuna->aspect-based-sentiment-analysis) (0.4.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2->aspect-based-sentiment-analysis) (3.1.0)\n",
            "\u001b[31mERROR: sentence-transformers 0.4.1.2 has requirement transformers<5.0.0,>=3.1.0, but you'll have transformers 2.5.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.10.1\n",
            "    Uninstalling tokenizers-0.10.1:\n",
            "      Successfully uninstalled tokenizers-0.10.1\n",
            "  Found existing installation: transformers 4.3.2\n",
            "    Uninstalling transformers-4.3.2:\n",
            "      Successfully uninstalled transformers-4.3.2\n",
            "Successfully installed tokenizers-0.5.0 transformers-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiLKlPOhqt3C"
      },
      "source": [
        "from kneed import KneeLocator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovod10qCqt1Y"
      },
      "source": [
        "import aspect_based_sentiment_analysis as absa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6vSu4v0RCct",
        "outputId": "31fcc04e-a98d-4cae-fc97-e5a210e6b8cc"
      },
      "source": [
        "df = pd.read_excel('SC_Posts_1.xls', header=None)\r\n",
        "df.columns = ['post']\r\n",
        "\r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17946, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMxV0PNRSpxF",
        "outputId": "7930419b-6601-44a4-c325-95f5c9cdd785"
      },
      "source": [
        "df = df[(df['post'].str.find('FALSE')==-1)]\r\n",
        "df = df[(df['post'].str.find('Comments')==-1)]\r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12274, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsiXLg2CSsXV",
        "outputId": "1574bb26-d92d-44c4-afae-e5be402b8d19"
      },
      "source": [
        "# function to preprocess speech\r\n",
        "def clean(text):\r\n",
        "    \r\n",
        "    # removing paragraph numbers\r\n",
        "    text = re.sub('[0-9]+.\\t','',str(text))\r\n",
        "    \r\n",
        "    #text.replace('[^a-zA-Z]', '')\r\n",
        "    #text.replace('[^\\w\\s]', '')\r\n",
        "    #text = re.sub('[^a-zA-Z]', ' ', text)\r\n",
        "    #text = re.sub(r'\\s+', ' ', text)\r\n",
        "    \r\n",
        "    # removing new line characters\r\n",
        "    text = re.sub('\\n ','',str(text))\r\n",
        "    text = re.sub('\\n',' ',str(text))\r\n",
        "    # removing apostrophes\r\n",
        "    text = re.sub(\"'s\",'',str(text))\r\n",
        "    # removing hyphens\r\n",
        "    text = re.sub(\"-\",' ',str(text))\r\n",
        "    text = re.sub(\"— \",'',str(text))\r\n",
        "    # removing quotation marks\r\n",
        "    text = re.sub('\\\"','',str(text))\r\n",
        "    # removing any reference to outside text\r\n",
        "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\r\n",
        "    \r\n",
        "    return text\r\n",
        "\r\n",
        "# preprocessing speeches\r\n",
        "df['post_clean'] = df['post'].apply(clean)\r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12274, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "6U5gjIBDSvKi",
        "outputId": "4062501e-0de5-4f9c-b5a9-ac38e90b11d5"
      },
      "source": [
        "def preprocess_sent(tweet):\r\n",
        "    processed_tweet = tweet\r\n",
        "    processed_tweet.replace('[^a-zA-Z]', '')\r\n",
        "    processed_tweet.replace('[^\\w\\s]', '')\r\n",
        "    processed_tweet = re.sub('[^a-zA-Z]', ' ', processed_tweet)\r\n",
        "    processed_tweet = re.sub(r'\\s+', ' ', processed_tweet)\r\n",
        "    return(processed_tweet)\r\n",
        "\r\n",
        "df['Processed_Sent'] = df['post_clean'].apply(lambda x: preprocess_sent(x))\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post</th>\n",
              "      <th>post_clean</th>\n",
              "      <th>Processed_Sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Shannon Murray Gormley</td>\n",
              "      <td>Shannon Murray Gormley</td>\n",
              "      <td>Shannon Murray Gormley</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Just wanted to let anyone know if you are able...</td>\n",
              "      <td>Just wanted to let anyone know if you are able...</td>\n",
              "      <td>Just wanted to let anyone know if you are able...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Alicia Gottschalk</td>\n",
              "      <td>Alicia Gottschalk</td>\n",
              "      <td>Alicia Gottschalk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>My hubby had it last week and it has done wond...</td>\n",
              "      <td>My hubby had it last week and it has done wond...</td>\n",
              "      <td>My hubby had it last week and it has done wond...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Laura Sutera</td>\n",
              "      <td>Laura Sutera</td>\n",
              "      <td>Laura Sutera</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                post  ...                                     Processed_Sent\n",
              "0                             Shannon Murray Gormley  ...                             Shannon Murray Gormley\n",
              "2  Just wanted to let anyone know if you are able...  ...  Just wanted to let anyone know if you are able...\n",
              "4                                  Alicia Gottschalk  ...                                  Alicia Gottschalk\n",
              "5  My hubby had it last week and it has done wond...  ...  My hubby had it last week and it has done wond...\n",
              "7                                       Laura Sutera  ...                                       Laura Sutera\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwIx89PlS1Za",
        "outputId": "ad88188a-a208-4a54-bfc6-edf9bbbdd68f"
      },
      "source": [
        "df = df[df['Processed_Sent'].str.strip().str.len()>0]\r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11557, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV19Yq0tS6LF"
      },
      "source": [
        "# load english language model\r\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIcCXCM2TngU"
      },
      "source": [
        "# function to retrieve FIRST person name from post\r\n",
        "\r\n",
        "def get_person_name(text):\r\n",
        "    \r\n",
        "    doc = nlp(text)\r\n",
        "    \r\n",
        "    sent = []\r\n",
        "    \r\n",
        "    persons_count = 0\r\n",
        "    person_names = []\r\n",
        "    prev_person_name_index = -1\r\n",
        "    token_count = 0\r\n",
        "    \r\n",
        "    for token in doc:\r\n",
        "        if (token.ent_type_=='PERSON'):\r\n",
        "          persons_count+= 1\r\n",
        "          if(prev_person_name_index == -1):\r\n",
        "            person_names.append(token.text)\r\n",
        "            prev_person_name_index = token_count\r\n",
        "          else:\r\n",
        "            if((token_count-prev_person_name_index)==1):\r\n",
        "              person_names.append(token.text)\r\n",
        "              prev_person_name_index = token_count\r\n",
        "        token_count += 1\r\n",
        "\r\n",
        "    person_name = ' '.join(person_names)\r\n",
        "\r\n",
        "    return(person_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9P92Ik1TUEu"
      },
      "source": [
        "# function for checking if post contains only person names\r\n",
        "def check_only_person_names(text):\r\n",
        "    \r\n",
        "    doc = nlp(text)\r\n",
        "    \r\n",
        "    sent = []\r\n",
        "    \r\n",
        "    persons_count = 0\r\n",
        "    for token in doc:\r\n",
        "        if (token.ent_type_=='PERSON'):\r\n",
        "          persons_count+= 1\r\n",
        "\r\n",
        "    return((persons_count == len(doc)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOQMpK6HXhPO"
      },
      "source": [
        "df['Person_Name'] = df['Processed_Sent'].apply(lambda x: get_person_name(x))\r\n",
        "df['Is_Only_Person_Name'] = df['Processed_Sent'].apply(lambda x: check_only_person_names(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "3Lf8lQTAYMv3",
        "outputId": "2aa18e5d-7968-4cdd-bffd-a6a2e47db243"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post</th>\n",
              "      <th>post_clean</th>\n",
              "      <th>Processed_Sent</th>\n",
              "      <th>Person_Name</th>\n",
              "      <th>Is_Only_Person_Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Shannon Murray Gormley</td>\n",
              "      <td>Shannon Murray Gormley</td>\n",
              "      <td>Shannon Murray Gormley</td>\n",
              "      <td>Shannon Murray Gormley</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Just wanted to let anyone know if you are able...</td>\n",
              "      <td>Just wanted to let anyone know if you are able...</td>\n",
              "      <td>Just wanted to let anyone know if you are able...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Alicia Gottschalk</td>\n",
              "      <td>Alicia Gottschalk</td>\n",
              "      <td>Alicia Gottschalk</td>\n",
              "      <td>Alicia Gottschalk</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>My hubby had it last week and it has done wond...</td>\n",
              "      <td>My hubby had it last week and it has done wond...</td>\n",
              "      <td>My hubby had it last week and it has done wond...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Laura Sutera</td>\n",
              "      <td>Laura Sutera</td>\n",
              "      <td>Laura Sutera</td>\n",
              "      <td>Laura Sutera</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                post  ... Is_Only_Person_Name\n",
              "0                             Shannon Murray Gormley  ...                True\n",
              "2  Just wanted to let anyone know if you are able...  ...               False\n",
              "4                                  Alicia Gottschalk  ...                True\n",
              "5  My hubby had it last week and it has done wond...  ...               False\n",
              "7                                       Laura Sutera  ...                True\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "av5cYqpEZ874",
        "outputId": "4fe99a54-d52d-4fca-8c53-d9aef8c81cdb"
      },
      "source": [
        "df.sample(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post</th>\n",
              "      <th>post_clean</th>\n",
              "      <th>Processed_Sent</th>\n",
              "      <th>Person_Name</th>\n",
              "      <th>Is_Only_Person_Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10249</th>\n",
              "      <td>Sally Billett same. Yesterday morning I was so...</td>\n",
              "      <td>Sally Billett same. Yesterday morning I was so...</td>\n",
              "      <td>Sally Billett same Yesterday morning I was so ...</td>\n",
              "      <td>Sally Billett</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17213</th>\n",
              "      <td>Donna Cordner , that's fine. But I'm also happ...</td>\n",
              "      <td>Donna Cordner , that fine. But I'm also happy ...</td>\n",
              "      <td>Donna Cordner that fine But I m also happy to ...</td>\n",
              "      <td>Donna Cordner</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6203</th>\n",
              "      <td>Best wishes to your family and hoping your mom...</td>\n",
              "      <td>Best wishes to your family and hoping your mom...</td>\n",
              "      <td>Best wishes to your family and hoping your mom...</td>\n",
              "      <td>Don</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9398</th>\n",
              "      <td>Peter Moore</td>\n",
              "      <td>Peter Moore</td>\n",
              "      <td>Peter Moore</td>\n",
              "      <td>Peter Moore</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17555</th>\n",
              "      <td>Questions about this.</td>\n",
              "      <td>Questions about this.</td>\n",
              "      <td>Questions about this</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9886</th>\n",
              "      <td>Michelle Bell well that works!!! I wish I had ...</td>\n",
              "      <td>Michelle Bell well that works!!! I wish I had ...</td>\n",
              "      <td>Michelle Bell well that works I wish I had one...</td>\n",
              "      <td>Michelle Bell</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5302</th>\n",
              "      <td>My heart is with your family!</td>\n",
              "      <td>My heart is with your family!</td>\n",
              "      <td>My heart is with your family</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4628</th>\n",
              "      <td>Mary Paul Taylor thank you! I can definitely a...</td>\n",
              "      <td>Mary Paul Taylor thank you! I can definitely a...</td>\n",
              "      <td>Mary Paul Taylor thank you I can definitely as...</td>\n",
              "      <td>Mary Paul Taylor</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6794</th>\n",
              "      <td>Didi Victorious Gray</td>\n",
              "      <td>Didi Victorious Gray</td>\n",
              "      <td>Didi Victorious Gray</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4497</th>\n",
              "      <td>Deborah Champagne</td>\n",
              "      <td>Deborah Champagne</td>\n",
              "      <td>Deborah Champagne</td>\n",
              "      <td>Deborah Champagne</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7689</th>\n",
              "      <td>Survivor Corps HQ</td>\n",
              "      <td>Survivor Corps HQ</td>\n",
              "      <td>Survivor Corps HQ</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15598</th>\n",
              "      <td>Amber Petrich</td>\n",
              "      <td>Amber Petrich</td>\n",
              "      <td>Amber Petrich</td>\n",
              "      <td>Amber Petrich</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13534</th>\n",
              "      <td>Patti Gilliano</td>\n",
              "      <td>Patti Gilliano</td>\n",
              "      <td>Patti Gilliano</td>\n",
              "      <td>Patti Gilliano</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16190</th>\n",
              "      <td>I'm sending you love and standing with you in ...</td>\n",
              "      <td>I'm sending you love and standing with you in ...</td>\n",
              "      <td>I m sending you love and standing with you in ...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15851</th>\n",
              "      <td>I had severe migraine, seizure, hypereflexia, ...</td>\n",
              "      <td>I had severe migraine, seizure, hypereflexia, ...</td>\n",
              "      <td>I had severe migraine seizure hypereflexia dys...</td>\n",
              "      <td>hypereflexia dystonia tachycardia</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12851</th>\n",
              "      <td>My dr called it in but I think you have the ri...</td>\n",
              "      <td>My dr called it in but I think you have the ri...</td>\n",
              "      <td>My dr called it in but I think you have the ri...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7094</th>\n",
              "      <td>Prayers</td>\n",
              "      <td>Prayers</td>\n",
              "      <td>Prayers</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9687</th>\n",
              "      <td>Deb Tasker</td>\n",
              "      <td>Deb Tasker</td>\n",
              "      <td>Deb Tasker</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6016</th>\n",
              "      <td>I pray healing and a miraculous healing over y...</td>\n",
              "      <td>I pray healing and a miraculous healing over y...</td>\n",
              "      <td>I pray healing and a miraculous healing over y...</td>\n",
              "      <td>Jesus Name</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13895</th>\n",
              "      <td>Jessica Flounders Ecton</td>\n",
              "      <td>Jessica Flounders Ecton</td>\n",
              "      <td>Jessica Flounders Ecton</td>\n",
              "      <td>Jessica Flounders Ecton</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    post  ... Is_Only_Person_Name\n",
              "10249  Sally Billett same. Yesterday morning I was so...  ...               False\n",
              "17213  Donna Cordner , that's fine. But I'm also happ...  ...               False\n",
              "6203   Best wishes to your family and hoping your mom...  ...               False\n",
              "9398                                         Peter Moore  ...                True\n",
              "17555                              Questions about this.  ...               False\n",
              "9886   Michelle Bell well that works!!! I wish I had ...  ...               False\n",
              "5302                       My heart is with your family!  ...               False\n",
              "4628   Mary Paul Taylor thank you! I can definitely a...  ...               False\n",
              "6794                                Didi Victorious Gray  ...               False\n",
              "4497                                   Deborah Champagne  ...                True\n",
              "7689                                   Survivor Corps HQ  ...               False\n",
              "15598                                      Amber Petrich  ...                True\n",
              "13534                                     Patti Gilliano  ...                True\n",
              "16190  I'm sending you love and standing with you in ...  ...               False\n",
              "15851  I had severe migraine, seizure, hypereflexia, ...  ...               False\n",
              "12851  My dr called it in but I think you have the ri...  ...               False\n",
              "7094                                             Prayers  ...               False\n",
              "9687                                          Deb Tasker  ...               False\n",
              "6016   I pray healing and a miraculous healing over y...  ...               False\n",
              "13895                            Jessica Flounders Ecton  ...                True\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJiWWDZ1aB1t",
        "outputId": "f8c2c3cc-55da-46f6-8703-91df95c40810"
      },
      "source": [
        "# Total number of people present in post (those who posted/commented + those referred in post by others)\r\n",
        "\r\n",
        "total_number_of_people = len(set(list(df['Person_Name'])))\r\n",
        "total_number_of_people"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE23tbmeae9L"
      },
      "source": [
        "# Removing those entries where only person names are present in post\r\n",
        "\r\n",
        "df = df[df['Is_Only_Person_Name']==False]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6B9mrfAgo04"
      },
      "source": [
        "# Removing duplicate posts\r\n",
        "\r\n",
        "df = df.drop_duplicates(subset = [\"Processed_Sent\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MX7sRpObxmq",
        "outputId": "0b43cd18-f8bf-4a4a-95fa-fe30a3fa6f46"
      },
      "source": [
        "# Final posts to be considered for similarities and further processing\r\n",
        "\r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5290, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "Ec8xKU1Vb5oi",
        "outputId": "e92378c2-f789-4fe8-9044-dd92fe17a98e"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post</th>\n",
              "      <th>post_clean</th>\n",
              "      <th>Processed_Sent</th>\n",
              "      <th>Person_Name</th>\n",
              "      <th>Is_Only_Person_Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13873</th>\n",
              "      <td>Marcy Buchanan they use it in Peru to prevent ...</td>\n",
              "      <td>Marcy Buchanan they use it in Peru to prevent ...</td>\n",
              "      <td>Marcy Buchanan they use it in Peru to prevent ...</td>\n",
              "      <td>Marcy Buchanan</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9008</th>\n",
              "      <td>#PrayingForYou ??</td>\n",
              "      <td>#PrayingForYou ??</td>\n",
              "      <td>PrayingForYou</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Adriane Hogan McCullagh first 10 days of symptoms</td>\n",
              "      <td>Adriane Hogan McCullagh first 10 days of symptoms</td>\n",
              "      <td>Adriane Hogan McCullagh first days of symptoms</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2800</th>\n",
              "      <td>I'm glad you're here to tell your story! Take ...</td>\n",
              "      <td>I'm glad you're here to tell your story! Take ...</td>\n",
              "      <td>I m glad you re here to tell your story Take i...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4511</th>\n",
              "      <td>It was also politically tabu at the time to sa...</td>\n",
              "      <td>It was also politically tabu at the time to sa...</td>\n",
              "      <td>It was also politically tabu at the time to sa...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4246</th>\n",
              "      <td>Somebody that came from Europe cough pretty cl...</td>\n",
              "      <td>Somebody that came from Europe cough pretty cl...</td>\n",
              "      <td>Somebody that came from Europe cough pretty cl...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15937</th>\n",
              "      <td>Jennifer Merryman Cartwright</td>\n",
              "      <td>Jennifer Merryman Cartwright</td>\n",
              "      <td>Jennifer Merryman Cartwright</td>\n",
              "      <td>Jennifer Merryman</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14982</th>\n",
              "      <td>Jayne Ann Mitchell how was your recovery after...</td>\n",
              "      <td>Jayne Ann Mitchell how was your recovery after...</td>\n",
              "      <td>Jayne Ann Mitchell how was your recovery after...</td>\n",
              "      <td>Jayne Ann Mitchell</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11884</th>\n",
              "      <td>it's actually for my sister, but we wanted to ...</td>\n",
              "      <td>it actually for my sister, but we wanted to be...</td>\n",
              "      <td>it actually for my sister but we wanted to be ...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9457</th>\n",
              "      <td>Phylica Jones Frankenberger if you dont take i...</td>\n",
              "      <td>Phylica Jones Frankenberger if you dont take i...</td>\n",
              "      <td>Phylica Jones Frankenberger if you dont take i...</td>\n",
              "      <td>Phylica Jones</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    post  ... Is_Only_Person_Name\n",
              "13873  Marcy Buchanan they use it in Peru to prevent ...  ...               False\n",
              "9008                                   #PrayingForYou ??  ...               False\n",
              "20     Adriane Hogan McCullagh first 10 days of symptoms  ...               False\n",
              "2800   I'm glad you're here to tell your story! Take ...  ...               False\n",
              "4511   It was also politically tabu at the time to sa...  ...               False\n",
              "4246   Somebody that came from Europe cough pretty cl...  ...               False\n",
              "15937                       Jennifer Merryman Cartwright  ...               False\n",
              "14982  Jayne Ann Mitchell how was your recovery after...  ...               False\n",
              "11884  it's actually for my sister, but we wanted to ...  ...               False\n",
              "9457   Phylica Jones Frankenberger if you dont take i...  ...               False\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skYpLGeTb_Y4"
      },
      "source": [
        "# Column to be utilized is Processed_Sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXBBtsapOM0H",
        "outputId": "857a3188-5884-4172-a7b1-b38ea4d452dc"
      },
      "source": [
        "unique_posts = list(df['Processed_Sent'])\r\n",
        "len(unique_posts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5290"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "R48BjvlgFNFM",
        "outputId": "b784ffd0-345e-4d9c-fe0d-d52384137426"
      },
      "source": [
        "from sentence_transformers import CrossEncoder\r\n",
        "model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-6231a37d0408>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cross-encoder/ms-marco-TinyBERT-L-2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, num_labels, max_length, device, tokenizer_args)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mclassifier_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchitectures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         config_dict, _ = PretrainedConfig.get_config_dict(\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_config_archive_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mALL_PRETRAINED_CONFIG_ARCHIVE_MAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         )\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m                     )\n\u001b[1;32m    240\u001b[0m                 )\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Model name 'cross-encoder/ms-marco-TinyBERT-L-2' was not found in model name list. We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/cross-encoder/ms-marco-TinyBERT-L-2/config.json' was a path, a model identifier, or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JY4k4umFQJL"
      },
      "source": [
        "queries = [\"received Infusion\", \"received antibodies\", \"received regeneron\", \"received eli lilly\", \"received bamlanivimab\"]\r\n",
        "passages = list(df['Processed_Sent'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRU6BUj9Fx-q"
      },
      "source": [
        "matching_queries_dict = {}\r\n",
        "matching_posts_count = 0\r\n",
        "\r\n",
        "#Search in a loop for the individual queries\r\n",
        "for query in queries:\r\n",
        "    matching_tuples = []\r\n",
        "    start_time = time.time()\r\n",
        "\r\n",
        "    #Concatenate the query and all passages and predict the scores for the pairs [query, passage]\r\n",
        "    model_inputs = [[query, passage] for passage in passages]\r\n",
        "    scores = model.predict(model_inputs)\r\n",
        "\r\n",
        "    #Sort the scores in decreasing order\r\n",
        "    results = [{'input': inp, 'score': score} for inp, score in zip(model_inputs, scores)]\r\n",
        "    results = sorted(results, key=lambda x: x['score'], reverse=True)\r\n",
        "\r\n",
        "    print(\"Query:\", query)\r\n",
        "    print(\"Search took {:.2f} seconds\".format(time.time() - start_time))\r\n",
        "\r\n",
        "    for hit in results:\r\n",
        "        if(float((\"{:.2f}\".format(hit['score']))) > 0):\r\n",
        "          matching_tuple = (hit['input'][1], float((\"{:.2f}\".format(hit['score']))))\r\n",
        "          matching_tuples.append(matching_tuple)\r\n",
        "\r\n",
        "    matching_queries_dict[query] = matching_tuples\r\n",
        "    matching_posts_count += len(matching_tuples)\r\n",
        "    print(\"Number of Matching Posts:\", len(matching_tuples))\r\n",
        "    print(\"==========\")\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "print('For ',len(queries),' Queries, Total Matching Posts: ',matching_posts_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzdhfB7PMSvc"
      },
      "source": [
        "matched_posts_dfs = []\r\n",
        "for query_matched in matching_queries_dict.keys():\r\n",
        "  matched_tuples = matching_queries_dict[query_matched]\r\n",
        "  matched_tuple_df = pd.DataFrame({'Match_Count':([x for x in range(len(matched_tuples))]), 'Processed_Sent':[i[0] for i in matched_tuples], 'Score':[i[1] for i in matched_tuples]})\r\n",
        "  matched_tuple_df.sort_values(by=['Score'], ascending=False, inplace=True)\r\n",
        "  matched_posts_dfs.append(matched_tuple_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu-QjWNfNbIE"
      },
      "source": [
        "# Find knee in every df and collect all matching posts (Processed_Sent) in a list\r\n",
        "\r\n",
        "matching_posts_list = []\r\n",
        "\r\n",
        "matched_df_count = 0\r\n",
        "for matched_post_df in matched_posts_dfs:\r\n",
        "  x = matched_post_df['Match_Count']\r\n",
        "  y = matched_post_df['Score']\r\n",
        "  elbow = -1\r\n",
        "  curve_type_direction_list = [('convex','decreasing'), ('convex''increasing'), ('concave','decreasing'), ('concave','increasing')]\r\n",
        "  for combination in curve_type_direction_list:\r\n",
        "    kn = KneeLocator(x, y, curve=combination[0], direction=combination[1])\r\n",
        "    if((kn.knee > 0) & (kn.knee < (len(x)-1))):\r\n",
        "      elbow = kn.knee\r\n",
        "      print('For DF: ', matched_df_count, ', Eblow at: ', elbow, ' for ', combination[0], '-', combination[1])\r\n",
        "      break\r\n",
        "  if(elbow > -1):\r\n",
        "    current_matching_posts = list(matched_post_df['Processed_Sent'][:elbow])\r\n",
        "    matching_posts_list.extend(current_matching_posts)\r\n",
        "  matched_df_count += 1\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "print('Total Number of Matching Posts after Elbow: ', len(matching_posts_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXxfm7UQRzC8"
      },
      "source": [
        "matching_posts_list = list(set(matching_posts_list))\r\n",
        "print('Total Number of Unique Matching Posts: ', len(matching_posts_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8aOKGWyR59g"
      },
      "source": [
        "list(matching_posts_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXSmGvjCSGpT"
      },
      "source": [
        "unique_posts_tuples = []\r\n",
        "\r\n",
        "for x in matching_posts_list:\r\n",
        "  post_count = 0\r\n",
        "  for y in df['Processed_Sent']:\r\n",
        "    if(x == y):\r\n",
        "      unique_posts_tuple = ((list(df['post'])[post_count]), x)\r\n",
        "      unique_posts_tuples.append(unique_posts_tuple)\r\n",
        "    post_count += 1\r\n",
        "\r\n",
        "unique_posts_df = pd.DataFrame({'Post':[i[0] for i in unique_posts_tuples], 'Processed_Sent':[i[1] for i in unique_posts_tuples]})\r\n",
        "unique_posts_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9ylbv5bUqRZ"
      },
      "source": [
        "unique_posts_df.to_csv('Unique_Matching_Posts_Antidodies_Taken.csv', index=False, columns=unique_posts_df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJsw3qxm1bYW"
      },
      "source": [
        "posts_containing_ref_to_people_taking_antibodies = list(unique_posts_df['Post'])\r\n",
        "processed_sent_containing_ref_to_people_taking_antibodies = list(unique_posts_df['Processed_Sent'])\r\n",
        "\r\n",
        "all_posts = list(df['post'])\r\n",
        "all_processed_sent = list(df['Processed_Sent'])\r\n",
        "\r\n",
        "remaining_posts = []\r\n",
        "remaining_processed_sent = []\r\n",
        "\r\n",
        "all_posts_count = 0\r\n",
        "for x in all_processed_sent:\r\n",
        "  if(x not in processed_sent_containing_ref_to_people_taking_antibodies):\r\n",
        "    remaining_processed_sent.append(x)\r\n",
        "    remaining_posts.append(all_posts[all_posts_count])\r\n",
        "  all_posts_count += 1\r\n",
        "\r\n",
        "print('All Posts Count: ', len(all_posts), '\\t', len(all_processed_sent))\r\n",
        "print('Posts containing antibodies received references Count: ', len(posts_containing_ref_to_people_taking_antibodies), '\\t', len(processed_sent_containing_ref_to_people_taking_antibodies))\r\n",
        "print('Remaining Posts Count: ', len(remaining_posts), '\\t', len(remaining_processed_sent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij95xqpks3Af"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4wnxKJK5QcY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSsNYubb5QRg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f1lHNs95Gkm"
      },
      "source": [
        "recognizer = absa.aux_models.BasicPatternRecognizer() # TODO - research for better/accurate pattern recognizer\r\n",
        "nlp = absa.load('absa/classifier-rest-0.2', pattern_recognizer=recognizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgs7WCRC8cRT"
      },
      "source": [
        "# Get Aspect based sentiments\r\n",
        "\r\n",
        "def get_ab_sentiments_df(posts_list, original_posts_list, aspects_list):\r\n",
        "  # PART 1\r\n",
        "  post_sentiments_list = []\r\n",
        "  posts_count = 0\r\n",
        "  for post in posts_list:\r\n",
        "    original_post = original_posts_list[posts_count]\r\n",
        "    completed_task = nlp(post, aspects=aspects_list)\r\n",
        "    total_neutral = 0\r\n",
        "    total_positive = 0\r\n",
        "    total_negative = 0\r\n",
        "    post_sentiments_dict = {}\r\n",
        "    for each_aspect in completed_task:\r\n",
        "      if(each_aspect.sentiment == absa.Sentiment.positive):\r\n",
        "        total_positive += 1\r\n",
        "        post_sentiments_dict[each_aspect.aspect] = 'Positive'\r\n",
        "      elif(each_aspect.sentiment == absa.Sentiment.negative):\r\n",
        "        total_negative += 1\r\n",
        "        post_sentiments_dict[each_aspect.aspect] = 'Negative'\r\n",
        "      else:\r\n",
        "        total_neutral += 1\r\n",
        "        post_sentiments_dict[each_aspect.aspect] = 'Neutral'\r\n",
        "\r\n",
        "    overall_sentiment = ''\r\n",
        "    if(total_neutral == len(aspects_list)):\r\n",
        "      overall_sentiment = 'Neutral'\r\n",
        "    elif(total_positive > total_negative):\r\n",
        "      overall_sentiment = 'Positive'\r\n",
        "    elif(total_negative > total_positive):\r\n",
        "      overall_sentiment = 'Negative'\r\n",
        "    else:\r\n",
        "      overall_sentiment = 'Negative'\r\n",
        "    \r\n",
        "    post_sentiments_dict['Overall_Sentiment'] = overall_sentiment\r\n",
        "    post_sentiments_list.append((original_post, post, post_sentiments_dict))\r\n",
        "    posts_count += 1\r\n",
        "\r\n",
        "  # PART 2\r\n",
        "  post_sentiments_tuples = []\r\n",
        "  # tuple struture: post, 'infusion', 'monoclonal antibodies', 'antibodies', 'regeneron', 'eli lilly', 'bamlanivimab' : use aspects_list\r\n",
        "\r\n",
        "  for post_sentiments_element in post_sentiments_list:\r\n",
        "    original_post = post_sentiments_element[0]\r\n",
        "    each_post = post_sentiments_element[1]\r\n",
        "    post_sentiments_dict = post_sentiments_element[2]\r\n",
        "\r\n",
        "    each_post_tuple = (original_post, each_post,)\r\n",
        "    for x in aspects_list:\r\n",
        "      for y in post_sentiments_dict.keys():\r\n",
        "        if(x == y):\r\n",
        "          each_post_tuple = each_post_tuple + (post_sentiments_dict[y],)\r\n",
        "          aspect_found = True\r\n",
        "          break\r\n",
        "\r\n",
        "    each_post_tuple = each_post_tuple + (post_sentiments_dict['Overall_Sentiment'],) \r\n",
        "    post_sentiments_tuples.append(each_post_tuple)\r\n",
        "\r\n",
        "  # PART 3\r\n",
        "  post_sentiments_df = pd.DataFrame({\r\n",
        "                                    'Post':[i[0] for i in post_sentiments_tuples],\r\n",
        "                                    'Processed_Sent':[i[1] for i in post_sentiments_tuples], \r\n",
        "                                    'Infusion':[i[2] for i in post_sentiments_tuples],\r\n",
        "                                    'Monoclonal_Antibodies':[i[3] for i in post_sentiments_tuples],\r\n",
        "                                    'Antibodies':[i[4] for i in post_sentiments_tuples],\r\n",
        "                                    'Regeneron':[i[5] for i in post_sentiments_tuples],\r\n",
        "                                    'Elililly':[i[6] for i in post_sentiments_tuples],\r\n",
        "                                    'Bamlanivimab':[i[7] for i in post_sentiments_tuples],\r\n",
        "                                    'Overall_Sentiment':[i[8] for i in post_sentiments_tuples]\r\n",
        "                                    })\r\n",
        "\r\n",
        "  return post_sentiments_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLIiuCblBnQl"
      },
      "source": [
        "# Get Overall sentiments\r\n",
        "\r\n",
        "def get_overall_sentiments_df(posts_list, original_posts_list):\r\n",
        "  posts_count = 0\r\n",
        "  post_sentiments_tuples = []\r\n",
        "  for post in posts_list:\r\n",
        "    original_post = original_posts_list[posts_count]\r\n",
        "    polarity = TextBlob(post).sentiment[0]\r\n",
        "    subjectivity = TextBlob(post).sentiment[1]\r\n",
        "\r\n",
        "    sentiment = ''\r\n",
        "    if(polarity ==0):\r\n",
        "      sentiment = 'Neutral'\r\n",
        "    elif(polarity < 0):\r\n",
        "      sentiment = 'Negative'\r\n",
        "    else:\r\n",
        "      sentiment = 'Positive'\r\n",
        "\r\n",
        "    each_post_tuple = (original_post, post, polarity, subjectivity, sentiment)\r\n",
        "    post_sentiments_tuples.append(each_post_tuple)\r\n",
        "\r\n",
        "  post_sentiments_df = pd.DataFrame({\r\n",
        "                                    'Post':[i[0] for i in post_sentiments_tuples],\r\n",
        "                                    'Processed_Sent':[i[1] for i in post_sentiments_tuples], \r\n",
        "                                    'Polarity':[i[2] for i in post_sentiments_tuples],\r\n",
        "                                    'Subjectivity':[i[3] for i in post_sentiments_tuples],\r\n",
        "                                    'Overall_Sentiment':[i[4] for i in post_sentiments_tuples]\r\n",
        "                                    })\r\n",
        "  \r\n",
        "  return post_sentiments_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOKjzRcKBm7E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lww7Y6oh8s_e"
      },
      "source": [
        "aspects_list = ['infusion', 'monoclonal antibodies', 'antibodies', 'regeneron', 'eli lilly', 'bamlanivimab']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzppNP6y6j1A"
      },
      "source": [
        "# Sentiment Analysis (aspect based) related to infusion/antibodies within posts that contain reference to people taking infusion/antibodies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpRqNAMq60Kn"
      },
      "source": [
        "absa_matching_posts_df = get_ab_sentiments_df(processed_sent_containing_ref_to_people_taking_antibodies, \r\n",
        "                                           posts_containing_ref_to_people_taking_antibodies, \r\n",
        "                                           aspects_list)\r\n",
        "\r\n",
        "absa_matching_posts_df.to_csv('AB_Sentiments_Matching_Posts.csv', index=False, columns=absa_matching_posts_df.columns)\r\n",
        "\r\n",
        "counter_absa_matching_posts=collections.Counter(absa_matching_posts_df['Overall_Sentiment'])\r\n",
        "plt.pie(counter_absa_matching_posts.values(), labels=counter_absa_matching_posts.keys(), autopct='%1.1i%%')\r\n",
        "plt.title(\"Sentiments Related to Antibodies/Infusion Within Posts Containing References of People Receiving Antibodies\", fontsize=14);\r\n",
        "pie.savefig(\"absa_matching_posts.png\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t32FSBq88YiV"
      },
      "source": [
        "# Sentiment Analysis (aspect based) related to infusion/antibodies within REMAINING posts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYtY6dUS8c35"
      },
      "source": [
        "absa_remaining_posts_df = get_ab_sentiments_df(remaining_processed_sent, \r\n",
        "                                           remaining_posts, \r\n",
        "                                           aspects_list)\r\n",
        "\r\n",
        "absa_remaining_posts_df.to_csv('AB_Sentiments_Remaining_Posts.csv', index=False, columns=absa_remaining_posts_df.columns)\r\n",
        "\r\n",
        "counter_absa_remaining_posts=collections.Counter(absa_remaining_posts_df['Overall_Sentiment'])\r\n",
        "plt.pie(counter_absa_remaining_posts.values(), labels=counter_absa_remaining_posts.keys(), autopct='%1.1i%%')\r\n",
        "plt.title(\"Sentiments Related to Antibodies/Infusion Within Posts Other Than Those Containing References of People Receiving Antibodies\", fontsize=14);\r\n",
        "pie.savefig(\"absa_remaining_posts.png\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX_3gwbI8eML"
      },
      "source": [
        "# Sentiment Analysis (aspect based) related to infusion/antibodies within ALL posts in corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZoKzPup8icP"
      },
      "source": [
        "absa_all_posts_df = get_ab_sentiments_df(all_processed_sent, \r\n",
        "                                           all_posts, \r\n",
        "                                           aspects_list)\r\n",
        "\r\n",
        "absa_all_posts_df.to_csv('AB_Sentiments_All_Posts.csv', index=False, columns=absa_all_posts_df.columns)\r\n",
        "\r\n",
        "counter_absa_all_posts=collections.Counter(absa_all_posts_df['Overall_Sentiment'])\r\n",
        "plt.pie(counter_absa_all_posts.values(), labels=counter_absa_all_posts.keys(), autopct='%1.1i%%')\r\n",
        "plt.title(\"Sentiments Related to Antibodies/Infusion Within All Posts\", fontsize=14);\r\n",
        "pie.savefig(\"absa_all_posts.png\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RVhighADLhO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Drll1IPO60dd"
      },
      "source": [
        "# Sentiment Analysis (overall) of posts containing reference to infusion/antibodies taken"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjOAA_vl77uf"
      },
      "source": [
        "overall_matching_posts_df = get_overall_sentiments_df(processed_sent_containing_ref_to_people_taking_antibodies, \r\n",
        "                                           posts_containing_ref_to_people_taking_antibodies)\r\n",
        "\r\n",
        "overall_matching_posts_df.to_csv('Overall_Sentiments_Matching_Posts.csv', index=False, columns=overall_matching_posts_df.columns)\r\n",
        "\r\n",
        "counter_overall_matching_posts=collections.Counter(overall_matching_posts_df['Overall_Sentiment'])\r\n",
        "plt.pie(counter_overall_matching_posts.values(), labels=counter_overall_matching_posts.keys(), autopct='%1.1i%%')\r\n",
        "plt.title(\"Overall Sentiments Within Posts Containing References of People Receiving Antibodies\", fontsize=14);\r\n",
        "pie.savefig(\"overall_matching_posts.png\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHM0cmtV8RtK"
      },
      "source": [
        "# Sentiment Analysis (overall) of REMANING posts (all - those containing reference to infusion/antibodies taken)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9oYPwSb8jxv"
      },
      "source": [
        "overall_remaining_posts_df = get_overall_sentiments_df(remaining_processed_sent, \r\n",
        "                                           remaining_posts)\r\n",
        "\r\n",
        "overall_remaining_posts_df.to_csv('Overall_Sentiments_Remaining_Posts.csv', index=False, columns=overall_remaining_posts_df.columns)\r\n",
        "\r\n",
        "counter_overall_remaining_posts=collections.Counter(overall_remaining_posts_df['Overall_Sentiment'])\r\n",
        "plt.pie(counter_overall_remaining_posts.values(), labels=counter_overall_remaining_posts.keys(), autopct='%1.1i%%')\r\n",
        "plt.title(\"Overall Sentiments Within Posts Other Than Those Containing References of People Receiving Antibodies\", fontsize=14);\r\n",
        "pie.savefig(\"overall_remaining_posts.png\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUysSboq778g"
      },
      "source": [
        "# Sentiment Analysis (overall) of ALL posts in corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZYJoYrc8Jmx"
      },
      "source": [
        "overall_all_posts_df = get_overall_sentiments_df(all_processed_sent, \r\n",
        "                                           all_posts)\r\n",
        "\r\n",
        "overall_all_posts_df.to_csv('Overall_Sentiments_All_Posts.csv', index=False, columns=overall_all_posts_df.columns)\r\n",
        "\r\n",
        "counter_overall_all_posts=collections.Counter(overall_all_posts_df['Overall_Sentiment'])\r\n",
        "plt.pie(counter_overall_all_posts.values(), labels=counter_overall_all_posts.keys(), autopct='%1.1i%%')\r\n",
        "plt.title(\"Overall Sentiments Within All Posts\", fontsize=14);\r\n",
        "pie.savefig(\"overall_all_posts.png\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWIdVYdxs6Fi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}